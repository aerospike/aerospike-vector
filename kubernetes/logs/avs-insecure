+ trap 'echo "Error: $? at line $LINENO" >&2' ERR
++ pwd
+ WORKSPACE=/home/joem/src/aerospike-vector/kubernetes
++ gcloud config get-value project
+ PROJECT_ID=performance-eco
++ whoami
+ USERNAME=joem
+ CHART_VERSION=0.7.0
+ DEFAULT_CLUSTER_NAME_SUFFIX=avs
+ [[ 3 -gt 0 ]]
+ case $1 in
+ RUN_INSECURE=1
+ shift
+ [[ 2 -gt 0 ]]
+ case $1 in
+ CLUSTER_NAME_OVERRIDE=avs-insecure2
+ shift 2
+ [[ 0 -gt 0 ]]
+ main
+ set_env_variables
+ '[' -n avs-insecure2 ']'
+ export CLUSTER_NAME=joem-avs-insecure2
+ CLUSTER_NAME=joem-avs-insecure2
+ export NODE_POOL_NAME_AEROSPIKE=aerospike-pool
+ NODE_POOL_NAME_AEROSPIKE=aerospike-pool
+ export NODE_POOL_NAME_AVS=avs-pool
+ NODE_POOL_NAME_AVS=avs-pool
+ export ZONE=us-central1-c
+ ZONE=us-central1-c
+ export FEATURES_CONF=/home/joem/src/aerospike-vector/kubernetes/features.conf
+ FEATURES_CONF=/home/joem/src/aerospike-vector/kubernetes/features.conf
+ export BUILD_DIR=/home/joem/src/aerospike-vector/kubernetes/generated
+ BUILD_DIR=/home/joem/src/aerospike-vector/kubernetes/generated
+ export REVERSE_DNS_AVS
+ print_env
+ echo 'Environment Variables:'
Environment Variables:
+ echo 'export PROJECT_ID=performance-eco'
export PROJECT_ID=performance-eco
+ echo 'export CLUSTER_NAME=joem-avs-insecure2'
export CLUSTER_NAME=joem-avs-insecure2
+ echo 'export NODE_POOL_NAME_AEROSPIKE=aerospike-pool'
export NODE_POOL_NAME_AEROSPIKE=aerospike-pool
+ echo 'export NODE_POOL_NAME_AVS=avs-pool'
export NODE_POOL_NAME_AVS=avs-pool
+ echo 'export ZONE=us-central1-c'
export ZONE=us-central1-c
+ echo 'export FEATURES_CONF=/home/joem/src/aerospike-vector/kubernetes/features.conf'
export FEATURES_CONF=/home/joem/src/aerospike-vector/kubernetes/features.conf
+ echo 'export CHART_LOCATION='
export CHART_LOCATION=
+ echo 'export RUN_INSECURE=1'
export RUN_INSECURE=1
+ reset_build
+ '[' -d /home/joem/src/aerospike-vector/kubernetes/generated ']'
++ mktemp -d /tmp/avs-deploy-previous.XXXXXX
+ temp_dir=/tmp/avs-deploy-previous.seR081
+ mv -f /home/joem/src/aerospike-vector/kubernetes/generated /tmp/avs-deploy-previous.seR081
+ mkdir -p /home/joem/src/aerospike-vector/kubernetes/generated/input /home/joem/src/aerospike-vector/kubernetes/generated/output /home/joem/src/aerospike-vector/kubernetes/generated/secrets /home/joem/src/aerospike-vector/kubernetes/generated/certs /home/joem/src/aerospike-vector/kubernetes/generated/manifests
+ cp /home/joem/src/aerospike-vector/kubernetes/features.conf /home/joem/src/aerospike-vector/kubernetes/generated/secrets/features.conf
+ docker run --rm -v /home/joem/src/aerospike-vector/kubernetes:/workdir -w /workdir mikefarah/yq e '.aerospikeVectorSearchConfig.cluster *= (load("manifests/avs-values-role-query.yaml"))' /workdir/manifests/avs-values.yaml
+ docker run --rm -v /home/joem/src/aerospike-vector/kubernetes:/workdir -w /workdir mikefarah/yq e '.aerospikeVectorSearchConfig.cluster *= (load("manifests/avs-values-role-update.yaml"))' /workdir/manifests/avs-values.yaml
+ cp /home/joem/src/aerospike-vector/kubernetes/manifests/aerospike-cr.yaml /home/joem/src/aerospike-vector/kubernetes/generated/manifests/
+ [[ 1 != 1 ]]
+ create_gke_cluster
+ gcloud container clusters describe joem-avs-insecure2 --zone us-central1-c
+ echo 'Cluster joem-avs-insecure2 does not exist. Creating...'
Cluster joem-avs-insecure2 does not exist. Creating...
++ date '+%Y-%m-%d %H:%M:%S'
+ echo '2024-12-05 16:57:32 - Starting GKE cluster creation...'
2024-12-05 16:57:32 - Starting GKE cluster creation...
+ gcloud container clusters create joem-avs-insecure2 --project performance-eco --zone us-central1-c --num-nodes 1 --disk-type pd-standard --disk-size 100
Note: The Kubelet readonly port (10255) is now deprecated. Please update your workloads to use the recommended alternatives. See https://cloud.google.com/kubernetes-engine/docs/how-to/disable-kubelet-readonly-port for ways to check usage and for migration instructions.
Note: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s).
Creating cluster joem-avs-insecure2 in us-central1-c...
.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.
Created [https://container.googleapis.com/v1/projects/performance-eco/zones/us-central1-c/clusters/joem-avs-insecure2].
To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-central1-c/joem-avs-insecure2?project=performance-eco
kubeconfig entry generated for joem-avs-insecure2.
NAME                LOCATION       MASTER_VERSION      MASTER_IP      MACHINE_TYPE  NODE_VERSION        NUM_NODES  STATUS
joem-avs-insecure2  us-central1-c  1.30.5-gke.1699000  35.188.115.25  e2-medium     1.30.5-gke.1699000  1          RUNNING
+ echo 'GKE cluster created successfully.'
GKE cluster created successfully.
+ echo 'Creating Aerospike node pool...'
Creating Aerospike node pool...
+ gcloud container node-pools create aerospike-pool --cluster joem-avs-insecure2 --project performance-eco --zone us-central1-c --num-nodes 3 --local-ssd-count 2 --disk-type pd-standard --disk-size 100 --machine-type n2d-standard-32
Creating node pool aerospike-pool...
.......................................................................................................................................................................................................................................................................................................................................................................................................................done.
Created [https://container.googleapis.com/v1/projects/performance-eco/zones/us-central1-c/clusters/joem-avs-insecure2/nodePools/aerospike-pool].
NAME            MACHINE_TYPE     DISK_SIZE_GB  NODE_VERSION
aerospike-pool  n2d-standard-32  100           1.30.5-gke.1699000
+ echo 'Aerospike node pool added successfully.'
Aerospike node pool added successfully.
+ echo 'Labeling Aerospike nodes...'
Labeling Aerospike nodes...
+ xargs -I '{}' kubectl label '{}' aerospike.com/node-pool=default-rack --overwrite
+ kubectl get nodes -l cloud.google.com/gke-nodepool=aerospike-pool -o name
node/gke-joem-avs-insecure2-aerospike-pool-e5f993df-c6nd labeled
node/gke-joem-avs-insecure2-aerospike-pool-e5f993df-g3np labeled
node/gke-joem-avs-insecure2-aerospike-pool-e5f993df-pxzw labeled
+ echo 'Adding AVS node pool...'
Adding AVS node pool...
+ gcloud container node-pools create avs-pool --cluster joem-avs-insecure2 --project performance-eco --zone us-central1-c --num-nodes 3 --disk-type pd-standard --disk-size 100 --machine-type n2d-standard-32
Creating node pool avs-pool...
.............................................................................................................................................................................................................................................................................................................................................................................................done.
Created [https://container.googleapis.com/v1/projects/performance-eco/zones/us-central1-c/clusters/joem-avs-insecure2/nodePools/avs-pool].
NAME      MACHINE_TYPE     DISK_SIZE_GB  NODE_VERSION
avs-pool  n2d-standard-32  100           1.30.5-gke.1699000
+ echo 'AVS node pool added successfully.'
AVS node pool added successfully.
+ echo 'Labeling AVS nodes...'
Labeling AVS nodes...
+ kubectl get nodes -l cloud.google.com/gke-nodepool=avs-pool -o name
+ xargs -I '{}' kubectl label '{}' aerospike.com/node-pool=avs --overwrite
node/gke-joem-avs-insecure2-avs-pool-30c30d84-5jmx labeled
node/gke-joem-avs-insecure2-avs-pool-30c30d84-5xhh labeled
node/gke-joem-avs-insecure2-avs-pool-30c30d84-lkrf labeled
+ echo 'Setting up namespaces...'
Setting up namespaces...
+ setup_aerospike
+ kubectl create namespace aerospike
namespace/aerospike created
+ echo 'Deploying Aerospike Kubernetes Operator (AKO)...'
Deploying Aerospike Kubernetes Operator (AKO)...
+ kubectl get ns olm
+ echo 'Installing OLM...'
Installing OLM...
+ curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.25.0/install.sh
+ bash -s v0.25.0
customresourcedefinition.apiextensions.k8s.io/catalogsources.operators.coreos.com created
customresourcedefinition.apiextensions.k8s.io/clusterserviceversions.operators.coreos.com created
customresourcedefinition.apiextensions.k8s.io/installplans.operators.coreos.com created
customresourcedefinition.apiextensions.k8s.io/olmconfigs.operators.coreos.com created
customresourcedefinition.apiextensions.k8s.io/operatorconditions.operators.coreos.com created
customresourcedefinition.apiextensions.k8s.io/operatorgroups.operators.coreos.com created
customresourcedefinition.apiextensions.k8s.io/operators.operators.coreos.com created
customresourcedefinition.apiextensions.k8s.io/subscriptions.operators.coreos.com created
customresourcedefinition.apiextensions.k8s.io/catalogsources.operators.coreos.com condition met
customresourcedefinition.apiextensions.k8s.io/clusterserviceversions.operators.coreos.com condition met
customresourcedefinition.apiextensions.k8s.io/installplans.operators.coreos.com condition met
customresourcedefinition.apiextensions.k8s.io/olmconfigs.operators.coreos.com condition met
customresourcedefinition.apiextensions.k8s.io/operatorconditions.operators.coreos.com condition met
customresourcedefinition.apiextensions.k8s.io/operatorgroups.operators.coreos.com condition met
customresourcedefinition.apiextensions.k8s.io/operators.operators.coreos.com condition met
customresourcedefinition.apiextensions.k8s.io/subscriptions.operators.coreos.com condition met
namespace/olm created
namespace/operators created
serviceaccount/olm-operator-serviceaccount created
clusterrole.rbac.authorization.k8s.io/system:controller:operator-lifecycle-manager created
clusterrolebinding.rbac.authorization.k8s.io/olm-operator-binding-olm created
olmconfig.operators.coreos.com/cluster created
deployment.apps/olm-operator created
deployment.apps/catalog-operator created
clusterrole.rbac.authorization.k8s.io/aggregate-olm-edit created
clusterrole.rbac.authorization.k8s.io/aggregate-olm-view created
operatorgroup.operators.coreos.com/global-operators created
operatorgroup.operators.coreos.com/olm-operators created
clusterserviceversion.operators.coreos.com/packageserver created
catalogsource.operators.coreos.com/operatorhubio-catalog created
Waiting for deployment "olm-operator" rollout to finish: 0 of 1 updated replicas are available...
deployment "olm-operator" successfully rolled out
deployment "catalog-operator" successfully rolled out
Package server phase: Installing
Package server phase: Succeeded
deployment "packageserver" successfully rolled out
+ kubectl get subscription my-aerospike-kubernetes-operator --namespace operators
+ echo 'Installing AKO subscription...'
Installing AKO subscription...
+ kubectl create -f https://operatorhub.io/install/aerospike-kubernetes-operator.yaml
subscription.operators.coreos.com/my-aerospike-kubernetes-operator created
+ echo 'Waiting for AKO to be ready...'
Waiting for AKO to be ready...
+ true
+ kubectl --namespace operators get deployment/aerospike-operator-controller-manager
+ echo 'AKO setup is still in progress...'
AKO setup is still in progress...
+ sleep 10
+ true
+ kubectl --namespace operators get deployment/aerospike-operator-controller-manager
+ echo 'AKO setup is still in progress...'
AKO setup is still in progress...
+ sleep 10
+ true
+ kubectl --namespace operators get deployment/aerospike-operator-controller-manager
+ echo 'AKO setup is still in progress...'
AKO setup is still in progress...
+ sleep 10
+ true
+ kubectl --namespace operators get deployment/aerospike-operator-controller-manager
+ echo 'AKO is ready.'
AKO is ready.
+ kubectl --namespace operators wait --for=condition=available --timeout=180s deployment/aerospike-operator-controller-manager
deployment.apps/aerospike-operator-controller-manager condition met
+ break
+ echo 'Granting permissions to the target namespace...'
Granting permissions to the target namespace...
+ kubectl --namespace aerospike create serviceaccount aerospike-operator-controller-manager
serviceaccount/aerospike-operator-controller-manager created
+ kubectl create clusterrolebinding aerospike-cluster --clusterrole=aerospike-cluster --serviceaccount=aerospike:aerospike-operator-controller-manager
clusterrolebinding.rbac.authorization.k8s.io/aerospike-cluster created
+ echo 'Setting secrets for Aerospike cluster...'
Setting secrets for Aerospike cluster...
+ kubectl --namespace aerospike create secret generic aerospike-secret --from-file=/home/joem/src/aerospike-vector/kubernetes/generated/secrets
secret/aerospike-secret created
+ kubectl --namespace aerospike create secret generic auth-secret --from-literal=password=admin123
secret/auth-secret created
+ kubectl --namespace aerospike create secret generic aerospike-tls --from-file=/home/joem/src/aerospike-vector/kubernetes/generated/certs
secret/aerospike-tls created
+ echo 'Adding storage class...'
Adding storage class...
+ kubectl apply -f https://raw.githubusercontent.com/aerospike/aerospike-kubernetes-operator/master/config/samples/storage/gce_ssd_storage_class.yaml
storageclass.storage.k8s.io/ssd created
+ echo 'Deploying Aerospike cluster...'
Deploying Aerospike cluster...
+ kubectl apply -f /home/joem/src/aerospike-vector/kubernetes/generated/manifests/aerospike-cr.yaml
aerospikecluster.asdb.aerospike.com/aerocluster created
+ deploy_istio
+ echo 'Deploying Istio'
Deploying Istio
+ helm repo add istio https://istio-release.storage.googleapis.com/charts
"istio" has been added to your repositories
+ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "jetstack" chart repository
...Successfully got an update from the "istio" chart repository
...Successfully got an update from the "aerospike-io" chart repository
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈Happy Helming!⎈
+ helm install istio-base istio/base --namespace istio-system --set defaultRevision=default --create-namespace --wait
NAME: istio-base
LAST DEPLOYED: Thu Dec  5 17:07:34 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Istio base successfully installed!

To learn more about the release, try:
  $ helm status istio-base -n istio-system
  $ helm get all istio-base -n istio-system
+ helm install istiod istio/istiod --namespace istio-system --create-namespace --wait
NAME: istiod
LAST DEPLOYED: Thu Dec  5 17:07:46 2024
NAMESPACE: istio-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
"istiod" successfully installed!

To learn more about the release, try:
  $ helm status istiod -n istio-system
  $ helm get all istiod -n istio-system

Next steps:
  * Deploy a Gateway: https://istio.io/latest/docs/setup/additional-setup/gateway/
  * Try out our tasks to get started on common configurations:
    * https://istio.io/latest/docs/tasks/traffic-management
    * https://istio.io/latest/docs/tasks/security/
    * https://istio.io/latest/docs/tasks/policy-enforcement/
  * Review the list of actively supported releases, CVE publications and our hardening guide:
    * https://istio.io/latest/docs/releases/supported-releases/
    * https://istio.io/latest/news/security/
    * https://istio.io/latest/docs/ops/best-practices/security/

For further documentation see https://istio.io website
+ helm install istio-ingress istio/gateway --values ./manifests/istio/istio-ingressgateway-values.yaml --namespace istio-ingress --create-namespace --wait
NAME: istio-ingress
LAST DEPLOYED: Thu Dec  5 17:07:59 2024
NAMESPACE: istio-ingress
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
"istio-ingress" successfully installed!

To learn more about the release, try:
  $ helm status istio-ingress -n istio-ingress
  $ helm get all istio-ingress -n istio-ingress

Next steps:
  * Deploy an HTTP Gateway: https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/
  * Deploy an HTTPS Gateway: https://istio.io/latest/docs/tasks/traffic-management/ingress/secure-ingress/
+ kubectl apply -f manifests/istio/gateway.yaml
gateway.networking.istio.io/avs-gw created
+ kubectl apply -f manifests/istio/avs-virtual-service.yaml
virtualservice.networking.istio.io/avs-vs created
+ get_reverse_dns
++ kubectl get svc istio-ingress -n istio-ingress -o 'jsonpath={.status.loadBalancer.ingress[0].ip}'
+ INGRESS_IP=34.28.28.145
++ dig +short -x 34.28.28.145
+ REVERSE_DNS_AVS=145.28.28.34.bc.googleusercontent.com.
+ echo 'Reverse DNS: 145.28.28.34.bc.googleusercontent.com.'
Reverse DNS: 145.28.28.34.bc.googleusercontent.com.
+ [[ 1 != 1 ]]
+ setup_avs
+ kubectl create namespace avs
namespace/avs created
+ echo 'Setting secrets for AVS cluster...'
Setting secrets for AVS cluster...
+ kubectl --namespace avs create secret generic auth-secret --from-literal=password=admin123
secret/auth-secret created
+ kubectl --namespace avs create secret generic aerospike-tls --from-file=/home/joem/src/aerospike-vector/kubernetes/generated/certs
secret/aerospike-tls created
+ kubectl --namespace avs create secret generic aerospike-secret --from-file=/home/joem/src/aerospike-vector/kubernetes/generated/secrets
secret/aerospike-secret created
+ deploy_avs_helm_chart
+ echo 'Deploying AVS Helm chart...'
Deploying AVS Helm chart...
+ helm repo add aerospike-helm https://artifact.aerospike.io/artifactory/api/helm/aerospike-helm
"aerospike-helm" has been added to your repositories
+ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "istio" chart repository
...Successfully got an update from the "jetstack" chart repository
...Successfully got an update from the "aerospike-io" chart repository
...Successfully got an update from the "aerospike-helm" chart repository
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈Happy Helming!⎈
+ helm install avs-app-query --set replicaCount=2 --values /home/joem/src/aerospike-vector/kubernetes/generated/manifests/avs-values-update.yaml --namespace avs aerospike-helm/aerospike-vector-search --version 0.7.0 --atomic --wait
NAME: avs-app-query
LAST DEPLOYED: Thu Dec  5 17:09:16 2024
NAMESPACE: avs
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:

+ helm install avs-app-update --set replicaCount=1 --values /home/joem/src/aerospike-vector/kubernetes/generated/manifests/avs-values-query.yaml --namespace avs aerospike-helm/aerospike-vector-search --version 0.7.0 --atomic --wait
NAME: avs-app-update
LAST DEPLOYED: Thu Dec  5 17:09:41 2024
NAMESPACE: avs
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:

+ setup_monitoring
+ echo 'Adding monitoring setup...'
Adding monitoring setup...
+ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
"prometheus-community" has been added to your repositories
+ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "istio" chart repository
...Successfully got an update from the "jetstack" chart repository
...Successfully got an update from the "aerospike-helm" chart repository
...Successfully got an update from the "aerospike-io" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈Happy Helming!⎈
+ helm install monitoring-stack prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace
NAME: monitoring-stack
LAST DEPLOYED: Thu Dec  5 17:10:10 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=monitoring-stack"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
+ echo 'Applying additional monitoring manifests...'
Applying additional monitoring manifests...
+ kubectl apply -f manifests/monitoring/aerospike-exporter-service.yaml
service/aerospike-exporter created
+ kubectl apply -f manifests/monitoring/aerospike-servicemonitor.yaml
servicemonitor.monitoring.coreos.com/aerospike-monitor created
+ kubectl apply -f manifests/monitoring/avs-servicemonitor.yaml
servicemonitor.monitoring.coreos.com/avs-monitor created
+ print_final_instructions
+ echo Your new deployment is available at 145.28.28.34.bc.googleusercontent.com..
Your new deployment is available at 145.28.28.34.bc.googleusercontent.com..
+ echo Check your deployment using our command line tool asvec available at https://github.com/aerospike/asvec.
Check your deployment using our command line tool asvec available at https://github.com/aerospike/asvec.
+ [[ 1 != 1 ]]
+ echo 'Setup Complete!'
Setup Complete!
