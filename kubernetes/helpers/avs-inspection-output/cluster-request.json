{
            "model": "gpt-4o",
            "temperature": 0.3,
            "messages": [
                {
                    "role": "system",
                    "content": "You are analyzing an Aerospike Vector Search cluster deployment.

Generate a comprehensive cluster analysis report with the following sections:

1. Resource Overview Table
   - Create a table showing each node's:
     * Total Memory (from node-aggregates.json)
     * Allocatable Memory (from node-aggregates.json)
     * AVS pods on node (with name and role from node-aggregates.json) and JVM Configuration:
     * Instance Type (from node-aggregates.json)
     * Status/Health (from node-aggregates.json)
2. Node Overview Table
    - Create a table showing details of each pod on each node:
        * Name
        * Roles
        * JVM Flags (extracted from full java command line in jvm-info.txt)
        * Memory Request
        * Memory Limit
        * Memory Used
3. Cluster-wide AVS information:
   * Cluster info:
   * Indecies:
4. Cluster Health Assessment
   - Memory distribution across nodes (using node-aggregates.json)
   - Resource allocation patterns (using node-aggregates.json)
   - GC pressure indicators (from pod JVM info)
   - Node conditions (from node-aggregates.json)

5. Key Metrics
   - Total cluster memory (sum from node-aggregates.json)
   - Average memory per AVS pod (calculated from node-aggregates.json)
   - Memory utilization percentages (calculated from node-aggregates.json)
   - Resource efficiency (calculated from node-aggregates.json)

6. Potential Issues
   - Memory pressure points (from node conditions and OOM events)
   - Resource imbalances (from node-aggregates.json)
   - GC concerns (from pod JVM info)
   - Configuration inconsistencies (from pod configs)

7. OOMKill Analysis
   - Detailed timeline of all OOMKill events found:
     * Container restart history
     * Previous termination states
     * System OOM events
     * Pod events
   
   - For each OOMKill event analyze:
     * JVM heap settings at the time
     * Node memory capacity
     * Whether it was an isolated incident or part of a pattern
     * Correlation with memory pressure or other events

8. Memory Configuration Assessment
   - Compare nodes/pods with and without OOMKills
   - Analyze if OOMKills correlate with:
     * Higher heap/node memory ratios
     * Specific workload patterns
     * Time of day or specific events
   
9. Recommendations
   - Specific memory configuration changes
   - System-level improvements
   - Monitoring enhancements
   - Prevention strategies

Provide a clear timeline of OOMKill events and their context.
Highlight patterns and potential root causes.

Pay special attention to:
- Pod restart counts and timing
- Last termination states and reasons
- Exit codes (137 indicates OOMKill)
- Time correlation between restarts and node pressure
- Pattern of restarts across the cluster
- Reletive uptime of the pods

When analyzing OOMKills:
1. Look at both container termination states AND system events
2. Check if restarts happened close to memory pressure events
3. Compare memory settings of pods that restarted vs stable pods
4. Consider the timing of restarts relative to pod age
5. Consider the relative uptime of the pods (if nodes restart it looks like 0 restarts)
Use the aggregated node information from node-aggregates.json to ensure accurate and consistent reporting of node resources, instance types, and cloud provider details.

Format the report exactly as shown in the example, with proper markdown formatting and consistent spacing. Please make use of emojis to make the report more engaging."
                },
                {
                    "role": "user",
                    "content": "Summary of Pod Restarts and OOMKills across cluster:
Pod: avs-app-aerospike-vector-search-0
Node: ip-192-168-52-147.ec2.internal
Restarts: 0
Age: 2025-04-29T19:17:11Z
Pod: avs-app-aerospike-vector-search-1
Node: ip-192-168-28-89.ec2.internal
Restarts: 0
Age: 2025-04-29T19:17:11Z
Pod: avs-app-aerospike-vector-search-2
Node: ip-192-168-27-123.ec2.internal
Restarts: 0
Age: 2025-04-29T19:17:11Z

Detailed Restart Analysis:

Node Memory Pressure Events:

Node: ip-192-168-26-117.ec2.internal
No memory pressure events found

Node: ip-192-168-27-123.ec2.internal
No memory pressure events found

Node: ip-192-168-28-89.ec2.internal
No memory pressure events found

Node: ip-192-168-52-147.ec2.internal
No memory pressure events found

Node: ip-192-168-53-124.ec2.internal
No memory pressure events found\n\nNode-Specific Analysis:\n
### Node: ip-192-168-26-117.ec2.internal
null

### Node: ip-192-168-27-123.ec2.internal
null

### Node: ip-192-168-28-89.ec2.internal
null

### Node: ip-192-168-52-147.ec2.internal
null

### Node: ip-192-168-53-124.ec2.internal
null"
                }
            ]
        }
